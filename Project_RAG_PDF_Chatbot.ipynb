{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amityadav108/Project-RAG-PDF-Chatbot/blob/main/Project_RAG_PDF_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cb3104-8bd1-4337-9d80-177aa4e6372a",
      "metadata": {
        "id": "74cb3104-8bd1-4337-9d80-177aa4e6372a"
      },
      "source": [
        "## PROJECT - RAG PDF Chatbot Using Uploaded PDF\n",
        "\n",
        "We will build:\n",
        "\n",
        "* Read your PDF\n",
        "* Split into small pieces\n",
        "* Create embeddings\n",
        "* Build a vector database\n",
        "* Ask questions and get answers\n",
        "* Add summary feature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa88783-b45a-446c-876f-8d6fbdfcd9c3",
      "metadata": {
        "id": "3fa88783-b45a-446c-876f-8d6fbdfcd9c3"
      },
      "source": [
        "### STEP - 1 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33589cb7-f53a-4a1d-9626-61cbbbdc13a9",
      "metadata": {
        "id": "33589cb7-f53a-4a1d-9626-61cbbbdc13a9",
        "outputId": "e3c31069-8a78-438b-8db0-51adce414760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (6.4.0)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (1.13.0)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Collecting langchain-core<2.0.0,>=1.0.6 (from langchain)\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
            "  Downloading langsmith-0.4.46-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading orjson-3.11.4-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
            "  Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: anyio in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain_community)\n",
            "  Downloading sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
            "  Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain_community)\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community)\n",
            "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\samsung\\miniconda3\\envs\\cleanbert\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
            "Downloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "Downloading langsmith-0.4.46-py3-none-any.whl (411 kB)\n",
            "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 0.8/2.0 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.8/2.0 MB 5.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 5.1 MB/s  0:00:00\n",
            "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 0.8/2.5 MB 4.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.4/2.5 MB 5.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.5/2.5 MB 5.2 MB/s  0:00:00\n",
            "Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.0/1.0 MB 8.3 MB/s  0:00:00\n",
            "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
            "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
            "Downloading sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ------------------------ --------------- 1.3/2.1 MB 6.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 7.0 MB/s  0:00:00\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading orjson-3.11.4-cp311-cp311-win_amd64.whl (131 kB)\n",
            "Downloading ormsgpack-1.12.0-cp311-cp311-win_amd64.whl (112 kB)\n",
            "Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
            "Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl (506 kB)\n",
            "Installing collected packages: zstandard, xxhash, typing-inspection, tenacity, python-dotenv, pydantic-core, propcache, ormsgpack, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, pydantic-settings, langsmith, langgraph-sdk, dataclasses-json, aiohttp, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langchain-classic, langgraph, langchain_community, langchain\n",
            "\n",
            "   -- -------------------------------------  2/37 [typing-inspection]\n",
            "   ---- -----------------------------------  4/37 [python-dotenv]\n",
            "   ----- ----------------------------------  5/37 [pydantic-core]\n",
            "   -------- -------------------------------  8/37 [orjson]\n",
            "   ----------- ---------------------------- 11/37 [marshmallow]\n",
            "   ------------ --------------------------- 12/37 [jsonpatch]\n",
            "   --------------- ------------------------ 14/37 [greenlet]\n",
            "   --------------- ------------------------ 14/37 [greenlet]\n",
            "   ------------------ --------------------- 17/37 [aiohappyeyeballs]\n",
            "   -------------------- ------------------- 19/37 [typing-inspect]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   --------------------- ------------------ 20/37 [SQLAlchemy]\n",
            "   ---------------------- ----------------- 21/37 [requests-toolbelt]\n",
            "   ---------------------- ----------------- 21/37 [requests-toolbelt]\n",
            "   ---------------------- ----------------- 21/37 [requests-toolbelt]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ----------------------- ---------------- 22/37 [pydantic]\n",
            "   ------------------------ --------------- 23/37 [aiosignal]\n",
            "   ------------------------- -------------- 24/37 [pydantic-settings]\n",
            "   --------------------------- ------------ 25/37 [langsmith]\n",
            "   --------------------------- ------------ 25/37 [langsmith]\n",
            "   --------------------------- ------------ 25/37 [langsmith]\n",
            "   --------------------------- ------------ 25/37 [langsmith]\n",
            "   --------------------------- ------------ 25/37 [langsmith]\n",
            "   ---------------------------- ----------- 26/37 [langgraph-sdk]\n",
            "   ----------------------------- ---------- 27/37 [dataclasses-json]\n",
            "   ------------------------------ --------- 28/37 [aiohttp]\n",
            "   ------------------------------ --------- 28/37 [aiohttp]\n",
            "   ------------------------------ --------- 28/37 [aiohttp]\n",
            "   ------------------------------ --------- 28/37 [aiohttp]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   ------------------------------- -------- 29/37 [langchain-core]\n",
            "   -------------------------------- ------- 30/37 [langgraph-checkpoint]\n",
            "   --------------------------------- ------ 31/37 [langchain-text-splitters]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ----------------------------------- ---- 33/37 [langchain-classic]\n",
            "   ------------------------------------ --- 34/37 [langgraph]\n",
            "   ------------------------------------ --- 34/37 [langgraph]\n",
            "   ------------------------------------ --- 34/37 [langgraph]\n",
            "   ------------------------------------ --- 34/37 [langgraph]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   ------------------------------------- -- 35/37 [langchain_community]\n",
            "   -------------------------------------- - 36/37 [langchain]\n",
            "   -------------------------------------- - 36/37 [langchain]\n",
            "   ---------------------------------------- 37/37 [langchain]\n",
            "\n",
            "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.2.4 httpx-sse-0.4.3 jsonpatch-1.33 langchain-1.0.8 langchain-classic-1.0.0 langchain-core-1.1.0 langchain-text-splitters-1.0.0 langchain_community-0.4.1 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 langsmith-0.4.46 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 orjson-3.11.4 ormsgpack-1.12.0 propcache-0.4.1 pydantic-2.12.4 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.2 xxhash-3.6.0 yarl-1.22.0 zstandard-0.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf sentence-transformers faiss-cpu langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f9cabe-adc5-4662-b901-2feb8fc65a62",
      "metadata": {
        "id": "65f9cabe-adc5-4662-b901-2feb8fc65a62"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328c23f3-56fd-40d4-9772-e0418b9e9b56",
      "metadata": {
        "id": "328c23f3-56fd-40d4-9772-e0418b9e9b56"
      },
      "source": [
        "### STEP - 2 Upload PDF File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67c8262-a819-47a9-b444-c8ec09595ab9",
      "metadata": {
        "id": "b67c8262-a819-47a9-b444-c8ec09595ab9",
        "outputId": "d1ef7ab1-9c52-4939-87b2-2e64280943a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF loaded Characters: 1987773\n"
          ]
        }
      ],
      "source": [
        "# This code:\n",
        "# Opens your PDF\n",
        "# Reads all pages\n",
        "# Stores everything inside a single text variable\n",
        "\n",
        "pdf_path = \"AI Foundations of Computational Agents_Pdf.pdf\"\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "        text += page_text + \"\\n\"\n",
        "\n",
        "print(\"PDF loaded Characters:\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228f4517-bc5c-4d10-a5cf-373acf34e8b5",
      "metadata": {
        "id": "228f4517-bc5c-4d10-a5cf-373acf34e8b5"
      },
      "source": [
        "### STEP - 3 Split Text Into Small Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e62a1cb-9d13-4ad1-b8fb-fe45eca7ea07",
      "metadata": {
        "id": "1e62a1cb-9d13-4ad1-b8fb-fe45eca7ea07",
        "outputId": "5721acc9-0491-4cf4-c7b4-898c11410216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 4418\n"
          ]
        }
      ],
      "source": [
        "# Chatbots can’t read huge text at once\n",
        "# So we split the PDF text into small pieces\n",
        "\n",
        "def make_chunks(text, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = make_chunks(text)\n",
        "print(\"Total chunks:\", len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2a48c2-6731-48d3-a847-a28ed5f3fcf9",
      "metadata": {
        "id": "db2a48c2-6731-48d3-a847-a28ed5f3fcf9"
      },
      "source": [
        "### STEP - 4 Convert Chunks Into Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4be96b1-b387-47f4-b13e-fc84dc1020a8",
      "metadata": {
        "id": "a4be96b1-b387-47f4-b13e-fc84dc1020a8",
        "outputId": "ded80b88-c365-4ecd-b5ac-5a59a348e5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (4418, 384)\n"
          ]
        }
      ],
      "source": [
        "# This converts text into numbers the AI can search through.\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "chunk_embeddings = model.encode(chunks)\n",
        "chunk_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
        "\n",
        "print(\"Embeddings shape:\", chunk_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7ffbf8-8375-4ddf-9dc2-10eb3ac4e397",
      "metadata": {
        "id": "be7ffbf8-8375-4ddf-9dc2-10eb3ac4e397"
      },
      "source": [
        "### STEP - 5 Store Embeddings in FAISS Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ab73a7-3dda-4e69-9aa1-389c682bf017",
      "metadata": {
        "id": "43ab73a7-3dda-4e69-9aa1-389c682bf017",
        "outputId": "9906ed68-fa12-4c99-bc94-d53bc52e60b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS Index Created \n"
          ]
        }
      ],
      "source": [
        "# FAISS helps us quickly find the most relevant text pieces.\n",
        "\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "print(\"FAISS Index Created \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb5e7bc-3f9c-4c5b-9540-3cc500aab130",
      "metadata": {
        "id": "1fb5e7bc-3f9c-4c5b-9540-3cc500aab130"
      },
      "source": [
        "### STEP - 6 Build the Question Anwering Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ede9a6-c39b-47ea-b1e6-ea6c14de6220",
      "metadata": {
        "id": "03ede9a6-c39b-47ea-b1e6-ea6c14de6220"
      },
      "outputs": [],
      "source": [
        "# This function:\n",
        "# Converts your question to an embedding\n",
        "# Searches similar text in FAISS\n",
        "# Returns the best 3 text chunks\n",
        "\n",
        "def ask_pdf(question):\n",
        "    q_embed = model.encode([question]).astype(\"float32\")\n",
        "    distances, result_ids = index.search(q_embed,3)\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for idx in result_ids[0]:\n",
        "        answer_text += chunks[idx] + \"\\n\\n\"\n",
        "\n",
        "    return answer_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a5143a1-06cd-4599-9d46-9fe679adb207",
      "metadata": {
        "id": "9a5143a1-06cd-4599-9d46-9fe679adb207"
      },
      "source": [
        "### STEP - 7 ASK Questions from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3ac245-4bb4-4246-af49-3efafbad821e",
      "metadata": {
        "id": "ff3ac245-4bb4-4246-af49-3efafbad821e",
        "outputId": "444f01bb-4567-4238-babc-8b24c71c0aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiﬁcial Intelligence?\n",
            "Artiﬁcial intelligence,o r AI, is the ﬁeld that studies the synthesis and analysis of\n",
            "computational agents that act intelligently. Consider each part of this deﬁnition.\n",
            "An agent is something that acts in an environment; it does something.\n",
            "Agents include worms, dogs, thermostats, airplanes, robots, humans, compa-\n",
            "nies, and countries.\n",
            "An agent is judged solely by how it acts. Agents that have the same effect\n",
            "in the world are equally good.\n",
            "3\n",
            "4 1. Artiﬁcial Intelligence and Age\n",
            "\n",
            "e on AI. In particular, Russell and Norvig\n",
            "[2020] give a more encyclopedic overview of AI. They provide an excellent\n",
            "complementary source for many of the topics covered in this book and also an\n",
            "outstanding review of the scientiﬁc literature, which we do not try to duplicate.\n",
            "The Association for the Advancement of Artiﬁcial Intelligence (AAAI) pro-\n",
            "vides introductory material and news at theirAI T opicswebsite (https://aitopics.\n",
            "org/). AI Magazine, published by AAAI, often has excellent overview \n",
            "\n",
            ". Remember that this book is not a survey of AI\n",
            "research.\n",
            "Acknowledgments\n",
            "Thanks to Saleema Amershi, Yoshua Bengio, Giuseppe Carenini, Jeff Clune,\n",
            "Mel Comisarow, Cristina Conati, Nando de Freitas, Rina Dechter, Bahare Fatemi,\n",
            "Randy Goebel, Robert Goldman, Jesse Hoey, Robert Holte, Christopher Mole,\n",
            "Kevin Murphy, Sriraam Natarajan, Alex Poole, Francesca Rossi, Justice Sefas,\n",
            "Ben Shneiderman, Peter van Beek, Geoffrey Woollard, and the anonymous re-\n",
            "viewers for valuable feedback on this third editi\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(ask_pdf(\"What is AI\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1537407e-d3dd-43fe-a2ee-edf408e6047b",
      "metadata": {
        "id": "1537407e-d3dd-43fe-a2ee-edf408e6047b",
        "outputId": "760f7ca2-4d64-4f7e-929f-6a8e08bb8087"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Networks \\n& Deep Learning\\n11: Causality\\n18: Social Impact of AI\\nFigure 1: Overview of chapters and dependencies\\nPart I\\nAgents in the World\\nWhat are Agents and How Can They be Built?\\nChapter 1\\nArtiﬁcial Intelligence and Agents\\nThe history of AI is a history of fantasies, possibilities, demonstrations,\\nand promise. Ever since Homer wrote of mechanical “tripods” waiting on\\nthe gods at dinner, imagined mechanical assistants have been a part of our\\nculture. However, only in the last half century hav\\n\\nd that has lost contact with what\\nAI should be about.”\\nGary Marcus, NYU, author ofRebooting AI\\n“Artiﬁcial Intelligence: Foundations of Computational Agentsskillfully delivers a compre-\\nhensive exploration of AI ideas, demonstrating exceptional organization and clarity of\\npresentation. Navigating the broad arc of important concepts and methods in AI, the\\nbook covers essential technical topics, historical context, and the growing importance\\nof the societal inﬂuences of AI, making it an outstanding\\n\\nulture. However, only in the last half century have we, the AI commu-\\nnity, been able to build experimental machines that test hypotheses about\\nthe mechanisms of thought and intelligent behavior and thereby demon-\\nstrate mechanisms that formerly existed only as theoretical possibilities.\\n– Bruce Buchanan [2005]\\nThis book is about artiﬁcial intelligence (AI), a ﬁeld built on centuries of thought,\\nwhich has been a recognized discipline for over 60 years. As well as solving\\npractical tasks, AI prov\\n\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ask_pdf(\"Explain the history of AI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a25a68-0aab-4761-85f6-09ba45cb0763",
      "metadata": {
        "id": "a4a25a68-0aab-4761-85f6-09ba45cb0763"
      },
      "source": [
        "### Step - 9 Add Summary Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bae7d6-c819-4d9f-aecd-68fae5a8842c",
      "metadata": {
        "id": "d9bae7d6-c819-4d9f-aecd-68fae5a8842c",
        "outputId": "348316b0-f49c-40e1-d850-cd869e801267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artiﬁcial Intelligence\n",
            "Foundations of Computational Agents\n",
            "Third Edition\n",
            "A comprehensive textbook for undergraduate and graduate AI courses, explaining\n",
            "modern artiﬁcial intelligence and its social impact, and integrating theory and prac-\n",
            "tice.  This extensively revised new edition now includes chapters on deep learning,\n",
            "including generative AI, the social impacts of AI, and causality. \n",
            "Students and instructors will beneﬁt from these features:\n",
            "• The novel agent design space, which provides a coherent framework for teaching\n",
            "and learning, making both easier. \n",
            " Every concept or algorithm is illustrated with a motivating concrete example. \n",
            " Each chapter now has a social impact section, enabling students to understand the\n",
            "impact of the various techniques as they learn.\n"
          ]
        }
      ],
      "source": [
        "def summerize_text(text):\n",
        "    sentences = text.split(\".\")\n",
        "    return \". \".join(sentences[:5]) + \".\"\n",
        "\n",
        "print(summerize_text(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5651e012-a9f4-48a5-9010-e7e9ddd4ae83",
      "metadata": {
        "id": "5651e012-a9f4-48a5-9010-e7e9ddd4ae83"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}